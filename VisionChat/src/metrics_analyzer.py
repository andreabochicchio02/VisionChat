"""
Metrics Analyzer for VisionChat.

This script reads the JSONL log files generated by the metrics loggers
and computes various performance metrics:

- Inference time (object detection)
- Time to process the first LLM request
- Time to process the second LLM request
- Total execution time
- Time taken by the LLM to send the request
- FPS (frames per second)
- FPS for classified images
- Per-unit metrics (per word, per token, per frame)

The script handles multiple log files from different processors:
- log_camera_metrics.jsonl: Camera/detection process metrics
- log_voice_metrics.jsonl: Voice assistant metrics
- log_llm_metrics.jsonl: LLM request metrics
"""

import json
import os
import sys
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict
import statistics


@dataclass
class MetricsSummary:
    """Container for computed metrics summary."""
    # Inference metrics
    avg_inference_time_ms: float = 0.0
    min_inference_time_ms: float = 0.0
    max_inference_time_ms: float = 0.0
    std_inference_time_ms: float = 0.0
    inference_time_per_object_ms: float = 0.0
    total_objects_detected: int = 0
    
    # LLM Request metrics
    first_llm_request_time_ms: float = 0.0
    second_llm_request_time_ms: float = 0.0
    avg_llm_request_time_ms: float = 0.0
    avg_time_to_first_token_ms: float = 0.0
    llm_time_per_word_ms: float = 0.0
    llm_time_per_token_ms: float = 0.0
    total_llm_requests: int = 0
    
    # Network timing
    avg_llm_send_time_ms: float = 0.0
    
    # FPS metrics
    avg_fps: float = 0.0
    min_fps: float = 0.0
    max_fps: float = 0.0
    avg_classification_fps: float = 0.0
    min_classification_fps: float = 0.0
    max_classification_fps: float = 0.0
    
    # Frame timing
    avg_frame_capture_time_ms: float = 0.0
    avg_frame_encoding_time_ms: float = 0.0
    avg_motion_detection_time_ms: float = 0.0
    
    # Interaction metrics
    avg_interaction_time_ms: float = 0.0
    avg_speech_recognition_time_ms: float = 0.0
    avg_tts_time_ms: float = 0.0
    interaction_time_per_word_ms: float = 0.0
    tts_time_per_word_ms: float = 0.0
    
    # Total execution
    total_session_duration_s: float = 0.0
    total_frames_processed: int = 0
    total_interactions: int = 0
    
    # Session info
    session_id: str = ""
    start_time: str = ""
    end_time: str = ""


class MetricsAnalyzer:
    """
    Analyzes metrics from JSONL log files.
    """
    
    def __init__(self, log_dir: str = "."):
        """
        Initialize the analyzer with the directory containing log files.
        
        Args:
            log_dir: Directory containing the JSONL log files
        """
        self.log_dir = log_dir
        self.camera_logs: List[Dict] = []
        self.voice_logs: List[Dict] = []
        self.llm_logs: List[Dict] = []
        
    def load_logs(self) -> None:
        """Load all log files from the log directory."""
        # Load camera metrics
        camera_log_path = os.path.join(self.log_dir, "log_camera_metrics.jsonl")
        if os.path.exists(camera_log_path):
            self.camera_logs = self._load_jsonl(camera_log_path)
            print(f"Loaded {len(self.camera_logs)} camera log entries")
        else:
            print(f"Warning: Camera log file not found: {camera_log_path}")
        
        # Load voice metrics
        voice_log_path = os.path.join(self.log_dir, "log_voice_metrics.jsonl")
        if os.path.exists(voice_log_path):
            self.voice_logs = self._load_jsonl(voice_log_path)
            print(f"Loaded {len(self.voice_logs)} voice log entries")
        else:
            print(f"Warning: Voice log file not found: {voice_log_path}")
        
        # Load LLM metrics
        llm_log_path = os.path.join(self.log_dir, "log_llm_metrics.jsonl")
        if os.path.exists(llm_log_path):
            self.llm_logs = self._load_jsonl(llm_log_path)
            print(f"Loaded {len(self.llm_logs)} LLM log entries")
        else:
            print(f"Warning: LLM log file not found: {llm_log_path}")
    
    def _load_jsonl(self, filepath: str) -> List[Dict]:
        """Load a JSONL file and return list of parsed entries."""
        entries = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        entries.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"Warning: Failed to parse line in {filepath}: {e}")
        return entries
    
    def _filter_by_event_type(self, logs: List[Dict], event_type: str) -> List[Dict]:
        """Filter logs by event type."""
        return [log for log in logs if log.get("event_type") == event_type]
    
    def _safe_stats(self, values: List[float]) -> Dict[str, float]:
        """Compute statistics safely, handling empty lists."""
        if not values:
            return {"avg": 0.0, "min": 0.0, "max": 0.0, "std": 0.0}
        
        return {
            "avg": statistics.mean(values),
            "min": min(values),
            "max": max(values),
            "std": statistics.stdev(values) if len(values) > 1 else 0.0
        }
    
    def compute_metrics(self, session_id: Optional[str] = None) -> MetricsSummary:
        """
        Compute all metrics from loaded logs.
        
        Args:
            session_id: Optional session ID to filter logs. If None, uses all logs.
            
        Returns:
            MetricsSummary with computed metrics
        """
        summary = MetricsSummary()
        
        # Filter by session if specified
        camera_logs = self.camera_logs
        voice_logs = self.voice_logs
        llm_logs = self.llm_logs
        
        if session_id:
            camera_logs = [l for l in camera_logs if l.get("session_id") == session_id]
            voice_logs = [l for l in voice_logs if l.get("session_id") == session_id]
            llm_logs = [l for l in llm_logs if l.get("session_id") == session_id]
            summary.session_id = session_id
        
        # Compute camera/detection metrics
        self._compute_camera_metrics(camera_logs, summary)
        
        # Compute LLM metrics
        self._compute_llm_metrics(llm_logs, summary)
        
        # Compute voice/interaction metrics
        self._compute_voice_metrics(voice_logs, summary)
        
        # Compute session timing
        self._compute_session_timing(camera_logs, voice_logs, llm_logs, summary)
        
        return summary
    
    def _compute_camera_metrics(self, logs: List[Dict], summary: MetricsSummary) -> None:
        """Compute camera and detection metrics."""
        
        # Object detection (inference) metrics
        detection_logs = self._filter_by_event_type(logs, "object_detection")
        if detection_logs:
            inference_times = [l.get("inference_time_ms", 0) for l in detection_logs]
            stats = self._safe_stats(inference_times)
            summary.avg_inference_time_ms = stats["avg"]
            summary.min_inference_time_ms = stats["min"]
            summary.max_inference_time_ms = stats["max"]
            summary.std_inference_time_ms = stats["std"]
            
            # Objects detected
            total_objects = sum(l.get("num_objects", 0) for l in detection_logs)
            summary.total_objects_detected = total_objects
            
            # Inference time per object
            if total_objects > 0:
                summary.inference_time_per_object_ms = sum(inference_times) / total_objects
        
        # FPS metrics
        fps_logs = self._filter_by_event_type(logs, "fps_metrics")
        if fps_logs:
            fps_values = [l.get("fps", 0) for l in fps_logs]
            classification_fps_values = [l.get("classification_fps", 0) for l in fps_logs]
            
            fps_stats = self._safe_stats(fps_values)
            summary.avg_fps = fps_stats["avg"]
            summary.min_fps = fps_stats["min"]
            summary.max_fps = fps_stats["max"]
            
            class_fps_stats = self._safe_stats(classification_fps_values)
            summary.avg_classification_fps = class_fps_stats["avg"]
            summary.min_classification_fps = class_fps_stats["min"]
            summary.max_classification_fps = class_fps_stats["max"]
        
        # Frame capture timing
        capture_logs = self._filter_by_event_type(logs, "frame_capture")
        if capture_logs:
            capture_times = [l.get("capture_time_ms", 0) for l in capture_logs]
            summary.avg_frame_capture_time_ms = statistics.mean(capture_times) if capture_times else 0.0
            summary.total_frames_processed = len(capture_logs)
        
        # Frame encoding timing
        encoding_logs = self._filter_by_event_type(logs, "frame_encoding")
        if encoding_logs:
            encoding_times = [l.get("encoding_time_ms", 0) for l in encoding_logs]
            summary.avg_frame_encoding_time_ms = statistics.mean(encoding_times) if encoding_times else 0.0
        
        # Motion detection timing
        motion_logs = self._filter_by_event_type(logs, "motion_detection")
        if motion_logs:
            motion_times = [l.get("motion_detection_time_ms", 0) for l in motion_logs]
            summary.avg_motion_detection_time_ms = statistics.mean(motion_times) if motion_times else 0.0
    
    def _compute_llm_metrics(self, logs: List[Dict], summary: MetricsSummary) -> None:
        """Compute LLM request metrics."""
        
        # LLM request end logs (contain total timing)
        request_end_logs = self._filter_by_event_type(logs, "llm_request_end")
        
        if request_end_logs:
            summary.total_llm_requests = len(request_end_logs)
            
            # Sort by timestamp to identify first and second requests
            sorted_requests = sorted(request_end_logs, key=lambda x: x.get("timestamp", 0))
            
            # First LLM request time
            if len(sorted_requests) >= 1:
                summary.first_llm_request_time_ms = sorted_requests[0].get("total_duration_ms", 0)
            
            # Second LLM request time
            if len(sorted_requests) >= 2:
                summary.second_llm_request_time_ms = sorted_requests[1].get("total_duration_ms", 0)
            
            # Average request time
            request_times = [l.get("total_duration_ms", 0) for l in request_end_logs]
            summary.avg_llm_request_time_ms = statistics.mean(request_times) if request_times else 0.0
            
            # Time per word
            total_words = sum(l.get("response_word_count", 0) for l in request_end_logs)
            total_time = sum(request_times)
            if total_words > 0:
                summary.llm_time_per_word_ms = total_time / total_words
            
            # Time per token
            total_tokens = sum(l.get("token_count", 0) for l in request_end_logs)
            if total_tokens > 0:
                summary.llm_time_per_token_ms = total_time / total_tokens
        
        # Time to first token
        first_token_logs = self._filter_by_event_type(logs, "llm_first_token")
        if first_token_logs:
            ttft_values = [l.get("time_to_first_token_ms", 0) for l in first_token_logs]
            summary.avg_time_to_first_token_ms = statistics.mean(ttft_values) if ttft_values else 0.0
        
        # Network timing (send time)
        network_logs = self._filter_by_event_type(logs, "llm_network_timing")
        if network_logs:
            send_times = [l.get("send_time_ms", 0) for l in network_logs]
            summary.avg_llm_send_time_ms = statistics.mean(send_times) if send_times else 0.0
    
    def _compute_voice_metrics(self, logs: List[Dict], summary: MetricsSummary) -> None:
        """Compute voice assistant and interaction metrics."""
        
        # Interaction metrics
        interaction_end_logs = self._filter_by_event_type(logs, "interaction_end")
        if interaction_end_logs:
            summary.total_interactions = len(interaction_end_logs)
            
            interaction_times = [l.get("total_duration_ms", 0) for l in interaction_end_logs]
            summary.avg_interaction_time_ms = statistics.mean(interaction_times) if interaction_times else 0.0
            
            # Interaction time per word
            total_words = sum(l.get("total_words", 0) for l in interaction_end_logs)
            total_time = sum(interaction_times)
            if total_words > 0:
                summary.interaction_time_per_word_ms = total_time / total_words
        
        # Speech recognition timing
        speech_end_logs = self._filter_by_event_type(logs, "speech_recognition_end")
        if speech_end_logs:
            recognition_times = [l.get("duration_ms", 0) for l in speech_end_logs]
            summary.avg_speech_recognition_time_ms = statistics.mean(recognition_times) if recognition_times else 0.0
        
        # TTS timing
        tts_end_logs = self._filter_by_event_type(logs, "tts_end")
        if tts_end_logs:
            tts_times = [l.get("duration_ms", 0) for l in tts_end_logs]
            summary.avg_tts_time_ms = statistics.mean(tts_times) if tts_times else 0.0
            
            # TTS time per word
            total_words = sum(l.get("word_count", 0) for l in tts_end_logs)
            total_time = sum(tts_times)
            if total_words > 0:
                summary.tts_time_per_word_ms = total_time / total_words
    
    def _compute_session_timing(self, camera_logs: List[Dict], voice_logs: List[Dict], 
                                 llm_logs: List[Dict], summary: MetricsSummary) -> None:
        """Compute total session timing."""
        all_logs = camera_logs + voice_logs + llm_logs
        
        if not all_logs:
            return
        
        # Get timestamps
        timestamps = [l.get("timestamp", 0) for l in all_logs if l.get("timestamp")]
        datetimes = [l.get("datetime", "") for l in all_logs if l.get("datetime")]
        
        if timestamps:
            summary.total_session_duration_s = max(timestamps) - min(timestamps)
        
        if datetimes:
            summary.start_time = min(datetimes)
            summary.end_time = max(datetimes)
        
        # Check for session end markers
        session_end_logs = [l for l in all_logs if l.get("event_type") == "session_end"]
        if session_end_logs:
            # Use the longest session duration
            durations = [l.get("total_session_duration_s", 0) for l in session_end_logs]
            if durations:
                summary.total_session_duration_s = max(durations)
    
    def get_sessions(self) -> List[str]:
        """Get list of unique session IDs from all logs."""
        all_logs = self.camera_logs + self.voice_logs + self.llm_logs
        session_ids = set()
        for log in all_logs:
            if log.get("session_id"):
                session_ids.add(log["session_id"])
        return sorted(list(session_ids))
    
    def print_summary(self, summary: MetricsSummary) -> None:
        """Print a formatted summary of the metrics."""
        print("\n" + "=" * 70)
        print("                     VISIONCHAT METRICS SUMMARY")
        print("=" * 70)
        
        if summary.session_id:
            print(f"\nSession ID: {summary.session_id}")
        if summary.start_time:
            print(f"Session Start: {summary.start_time}")
        if summary.end_time:
            print(f"Session End: {summary.end_time}")
        print(f"Total Session Duration: {summary.total_session_duration_s:.2f} seconds")
        
        print("\n" + "-" * 70)
        print("                    OBJECT DETECTION / INFERENCE")
        print("-" * 70)
        print(f"  Average Inference Time:      {summary.avg_inference_time_ms:.2f} ms")
        print(f"  Min Inference Time:          {summary.min_inference_time_ms:.2f} ms")
        print(f"  Max Inference Time:          {summary.max_inference_time_ms:.2f} ms")
        print(f"  Std Dev Inference Time:      {summary.std_inference_time_ms:.2f} ms")
        print(f"  Total Objects Detected:      {summary.total_objects_detected}")
        print(f"  Inference Time per Object:   {summary.inference_time_per_object_ms:.2f} ms/object")
        
        print("\n" + "-" * 70)
        print("                         FPS METRICS")
        print("-" * 70)
        print(f"  Average FPS:                 {summary.avg_fps:.2f}")
        print(f"  Min FPS:                     {summary.min_fps:.2f}")
        print(f"  Max FPS:                     {summary.max_fps:.2f}")
        print(f"  Average Classification FPS:  {summary.avg_classification_fps:.2f}")
        print(f"  Min Classification FPS:      {summary.min_classification_fps:.2f}")
        print(f"  Max Classification FPS:      {summary.max_classification_fps:.2f}")
        print(f"  Total Frames Processed:      {summary.total_frames_processed}")
        
        print("\n" + "-" * 70)
        print("                      FRAME PROCESSING")
        print("-" * 70)
        print(f"  Avg Frame Capture Time:      {summary.avg_frame_capture_time_ms:.2f} ms")
        print(f"  Avg Frame Encoding Time:     {summary.avg_frame_encoding_time_ms:.2f} ms")
        print(f"  Avg Motion Detection Time:   {summary.avg_motion_detection_time_ms:.2f} ms")
        
        print("\n" + "-" * 70)
        print("                       LLM REQUEST METRICS")
        print("-" * 70)
        print(f"  Total LLM Requests:          {summary.total_llm_requests}")
        print(f"  First LLM Request Time:      {summary.first_llm_request_time_ms:.2f} ms")
        print(f"  Second LLM Request Time:     {summary.second_llm_request_time_ms:.2f} ms")
        print(f"  Average LLM Request Time:    {summary.avg_llm_request_time_ms:.2f} ms")
        print(f"  Avg Time to First Token:     {summary.avg_time_to_first_token_ms:.2f} ms")
        print(f"  Avg LLM Send Time:           {summary.avg_llm_send_time_ms:.2f} ms")
        print(f"  LLM Time per Word:           {summary.llm_time_per_word_ms:.2f} ms/word")
        print(f"  LLM Time per Token:          {summary.llm_time_per_token_ms:.2f} ms/token")
        
        print("\n" + "-" * 70)
        print("                    INTERACTION METRICS")
        print("-" * 70)
        print(f"  Total Interactions:          {summary.total_interactions}")
        print(f"  Avg Interaction Time:        {summary.avg_interaction_time_ms:.2f} ms")
        print(f"  Avg Speech Recognition Time: {summary.avg_speech_recognition_time_ms:.2f} ms")
        print(f"  Avg TTS Time:                {summary.avg_tts_time_ms:.2f} ms")
        print(f"  Interaction Time per Word:   {summary.interaction_time_per_word_ms:.2f} ms/word")
        print(f"  TTS Time per Word:           {summary.tts_time_per_word_ms:.2f} ms/word")
        
        print("\n" + "=" * 70)
    
    def export_to_json(self, summary: MetricsSummary, output_path: str) -> None:
        """Export metrics summary to a JSON file."""
        # Convert dataclass to dict
        summary_dict = {
            "session_info": {
                "session_id": summary.session_id,
                "start_time": summary.start_time,
                "end_time": summary.end_time,
                "total_session_duration_s": summary.total_session_duration_s,
            },
            "inference_metrics": {
                "avg_inference_time_ms": summary.avg_inference_time_ms,
                "min_inference_time_ms": summary.min_inference_time_ms,
                "max_inference_time_ms": summary.max_inference_time_ms,
                "std_inference_time_ms": summary.std_inference_time_ms,
                "inference_time_per_object_ms": summary.inference_time_per_object_ms,
                "total_objects_detected": summary.total_objects_detected,
            },
            "fps_metrics": {
                "avg_fps": summary.avg_fps,
                "min_fps": summary.min_fps,
                "max_fps": summary.max_fps,
                "avg_classification_fps": summary.avg_classification_fps,
                "min_classification_fps": summary.min_classification_fps,
                "max_classification_fps": summary.max_classification_fps,
                "total_frames_processed": summary.total_frames_processed,
            },
            "frame_processing_metrics": {
                "avg_frame_capture_time_ms": summary.avg_frame_capture_time_ms,
                "avg_frame_encoding_time_ms": summary.avg_frame_encoding_time_ms,
                "avg_motion_detection_time_ms": summary.avg_motion_detection_time_ms,
            },
            "llm_metrics": {
                "total_llm_requests": summary.total_llm_requests,
                "first_llm_request_time_ms": summary.first_llm_request_time_ms,
                "second_llm_request_time_ms": summary.second_llm_request_time_ms,
                "avg_llm_request_time_ms": summary.avg_llm_request_time_ms,
                "avg_time_to_first_token_ms": summary.avg_time_to_first_token_ms,
                "avg_llm_send_time_ms": summary.avg_llm_send_time_ms,
                "llm_time_per_word_ms": summary.llm_time_per_word_ms,
                "llm_time_per_token_ms": summary.llm_time_per_token_ms,
            },
            "interaction_metrics": {
                "total_interactions": summary.total_interactions,
                "avg_interaction_time_ms": summary.avg_interaction_time_ms,
                "avg_speech_recognition_time_ms": summary.avg_speech_recognition_time_ms,
                "avg_tts_time_ms": summary.avg_tts_time_ms,
                "interaction_time_per_word_ms": summary.interaction_time_per_word_ms,
                "tts_time_per_word_ms": summary.tts_time_per_word_ms,
            },
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(summary_dict, f, indent=2)
        
        print(f"\nMetrics exported to: {output_path}")
    
    def get_per_unit_breakdown(self) -> Dict[str, Any]:
        """
        Get detailed per-unit metrics breakdown.
        
        Returns a dictionary with metrics broken down per unit:
        - Per word
        - Per token
        - Per frame
        - Per object
        - Per interaction
        """
        breakdown = {
            "per_word": {},
            "per_token": {},
            "per_frame": {},
            "per_object": {},
            "per_interaction": {},
        }
        
        # LLM per word/token metrics
        llm_end_logs = self._filter_by_event_type(self.llm_logs, "llm_request_end")
        for i, log in enumerate(llm_end_logs, 1):
            word_count = log.get("response_word_count", 0)
            token_count = log.get("token_count", 0)
            duration = log.get("total_duration_ms", 0)
            
            breakdown["per_word"][f"llm_request_{i}"] = {
                "word_count": word_count,
                "total_time_ms": duration,
                "time_per_word_ms": duration / word_count if word_count > 0 else 0,
            }
            
            breakdown["per_token"][f"llm_request_{i}"] = {
                "token_count": token_count,
                "total_time_ms": duration,
                "time_per_token_ms": duration / token_count if token_count > 0 else 0,
            }
        
        # Camera per frame metrics
        capture_logs = self._filter_by_event_type(self.camera_logs, "frame_capture")
        detection_logs = self._filter_by_event_type(self.camera_logs, "object_detection")
        
        total_capture_time = sum(l.get("capture_time_ms", 0) for l in capture_logs)
        total_frames = len(capture_logs)
        
        breakdown["per_frame"]["capture"] = {
            "total_frames": total_frames,
            "total_time_ms": total_capture_time,
            "avg_time_per_frame_ms": total_capture_time / total_frames if total_frames > 0 else 0,
        }
        
        # Per object metrics from detections
        total_inference_time = sum(l.get("inference_time_ms", 0) for l in detection_logs)
        total_objects = sum(l.get("num_objects", 0) for l in detection_logs)
        
        breakdown["per_object"]["detection"] = {
            "total_objects": total_objects,
            "total_inference_time_ms": total_inference_time,
            "avg_time_per_object_ms": total_inference_time / total_objects if total_objects > 0 else 0,
        }
        
        # Per interaction metrics
        interaction_logs = self._filter_by_event_type(self.voice_logs, "interaction_end")
        for i, log in enumerate(interaction_logs, 1):
            total_words = log.get("total_words", 0)
            duration = log.get("total_duration_ms", 0)
            
            breakdown["per_interaction"][f"interaction_{i}"] = {
                "total_words": total_words,
                "user_words": log.get("user_word_count", 0),
                "response_words": log.get("response_word_count", 0),
                "total_time_ms": duration,
                "time_per_word_ms": duration / total_words if total_words > 0 else 0,
                "success": log.get("success", False),
            }
        
        return breakdown


def main():
    """Main entry point for the metrics analyzer."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Analyze VisionChat performance metrics")
    parser.add_argument(
        "--log-dir", "-d",
        default=".",
        help="Directory containing the log files (default: current directory)"
    )
    parser.add_argument(
        "--session", "-s",
        default=None,
        help="Specific session ID to analyze (default: all sessions)"
    )
    parser.add_argument(
        "--output", "-o",
        default=None,
        help="Output JSON file for metrics export"
    )
    parser.add_argument(
        "--breakdown", "-b",
        action="store_true",
        help="Show detailed per-unit breakdown"
    )
    parser.add_argument(
        "--list-sessions", "-l",
        action="store_true",
        help="List available session IDs"
    )
    
    args = parser.parse_args()
    
    # Initialize analyzer
    analyzer = MetricsAnalyzer(args.log_dir)
    analyzer.load_logs()
    
    # List sessions if requested
    if args.list_sessions:
        sessions = analyzer.get_sessions()
        print("\nAvailable Sessions:")
        for session in sessions:
            print(f"  - {session}")
        return
    
    # Compute metrics
    summary = analyzer.compute_metrics(args.session)
    
    # Print summary
    analyzer.print_summary(summary)
    
    # Show breakdown if requested
    if args.breakdown:
        breakdown = analyzer.get_per_unit_breakdown()
        print("\n" + "=" * 70)
        print("                    PER-UNIT METRICS BREAKDOWN")
        print("=" * 70)
        print(json.dumps(breakdown, indent=2))
    
    # Export to JSON if requested
    if args.output:
        analyzer.export_to_json(summary, args.output)


if __name__ == "__main__":
    main()
